{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_list = []\n",
    "testing_list = []\n",
    "unsupervised_list = []\n",
    "\n",
    "with open(\"resources/train-pos.txt\",encoding='utf-8') as fid:\n",
    "    for line in fid:\n",
    "        training_list.append(line)\n",
    "        \n",
    "with open(\"resources/train-neg.txt\",encoding='utf-8') as fid:\n",
    "    for line in fid:\n",
    "        training_list.append(line)\n",
    "        \n",
    "with open(\"resources/test-pos.txt\",encoding='utf-8') as fid:\n",
    "    for line in fid:\n",
    "        testing_list.append(line)\n",
    "\n",
    "with open(\"resources/test-neg.txt\",encoding='utf-8') as fid:\n",
    "    for line in fid:\n",
    "        testing_list.append(line) \n",
    "        \n",
    "with open(\"resources/train-unsup.txt\",encoding='utf-8') as fid:\n",
    "    for line in fid:\n",
    "        unsupervised_list.append(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash\\Anaconda3\\lib\\site-packages\\gensim-1.0.1-py3.6-win-amd64.egg\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-04-03 09:29:56,935 : INFO : collecting all words and their counts\n",
      "2017-04-03 09:29:56,937 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec generation started ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-03 09:29:57,716 : INFO : PROGRESS: at sentence #10000, processed 2390666 words, keeping 49770 word types\n",
      "2017-04-03 09:29:58,422 : INFO : PROGRESS: at sentence #20000, processed 4731654 words, keeping 67469 word types\n",
      "2017-04-03 09:29:59,170 : INFO : PROGRESS: at sentence #30000, processed 7088415 words, keeping 80632 word types\n",
      "2017-04-03 09:29:59,976 : INFO : PROGRESS: at sentence #40000, processed 9381192 words, keeping 91668 word types\n",
      "2017-04-03 09:30:00,783 : INFO : PROGRESS: at sentence #50000, processed 11708957 words, keeping 100673 word types\n",
      "2017-04-03 09:30:01,505 : INFO : PROGRESS: at sentence #60000, processed 14109451 words, keeping 109920 word types\n",
      "2017-04-03 09:30:02,288 : INFO : PROGRESS: at sentence #70000, processed 16471157 words, keeping 118004 word types\n",
      "2017-04-03 09:30:03,313 : INFO : PROGRESS: at sentence #80000, processed 18840695 words, keeping 125686 word types\n",
      "2017-04-03 09:30:04,084 : INFO : PROGRESS: at sentence #90000, processed 21221705 words, keeping 132795 word types\n",
      "2017-04-03 09:30:04,750 : INFO : collected 139893 word types from a corpus of 23588303 raw words and 100000 sentences\n",
      "2017-04-03 09:30:04,750 : INFO : Loading a fresh vocabulary\n",
      "2017-04-03 09:30:05,548 : INFO : min_count=5 retains 53263 unique words (38% of original 139893, drops 86630)\n",
      "2017-04-03 09:30:05,549 : INFO : min_count=5 leaves 23450058 word corpus (99% of original 23588303, drops 138245)\n",
      "2017-04-03 09:30:06,615 : INFO : deleting the raw counts dictionary of 139893 items\n",
      "2017-04-03 09:30:06,627 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-04-03 09:30:06,631 : INFO : downsampling leaves estimated 17563525 word corpus (74.9% of prior 23450058)\n",
      "2017-04-03 09:30:06,637 : INFO : estimated required memory for 53263 words and 100 dimensions: 69241900 bytes\n",
      "2017-04-03 09:30:07,972 : INFO : resetting layer weights\n",
      "2017-04-03 09:30:10,030 : INFO : training model with 7 workers on 53263 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-04-03 09:30:10,030 : INFO : expecting 100000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-03 09:30:11,651 : INFO : PROGRESS: at 0.28% examples, 243486 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:12,673 : INFO : PROGRESS: at 1.05% examples, 457114 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:13,690 : INFO : PROGRESS: at 1.83% examples, 529087 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:14,697 : INFO : PROGRESS: at 2.44% examples, 534416 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:30:15,698 : INFO : PROGRESS: at 3.21% examples, 559106 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:16,700 : INFO : PROGRESS: at 3.93% examples, 569563 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:17,702 : INFO : PROGRESS: at 4.64% examples, 579972 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:18,745 : INFO : PROGRESS: at 5.34% examples, 579191 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:19,748 : INFO : PROGRESS: at 5.85% examples, 563404 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:30:20,784 : INFO : PROGRESS: at 6.44% examples, 556667 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:21,792 : INFO : PROGRESS: at 7.15% examples, 559805 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:22,794 : INFO : PROGRESS: at 7.77% examples, 558213 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:23,796 : INFO : PROGRESS: at 8.34% examples, 551776 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:24,810 : INFO : PROGRESS: at 8.83% examples, 541812 words/s, in_qsize 14, out_qsize 2\n",
      "2017-04-03 09:30:25,819 : INFO : PROGRESS: at 9.52% examples, 545550 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:26,858 : INFO : PROGRESS: at 10.06% examples, 540473 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:27,866 : INFO : PROGRESS: at 10.58% examples, 534723 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:28,874 : INFO : PROGRESS: at 11.20% examples, 535751 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:30:29,876 : INFO : PROGRESS: at 11.71% examples, 531720 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:30,878 : INFO : PROGRESS: at 12.08% examples, 522412 words/s, in_qsize 13, out_qsize 2\n",
      "2017-04-03 09:30:31,878 : INFO : PROGRESS: at 12.75% examples, 526060 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:32,897 : INFO : PROGRESS: at 13.36% examples, 526288 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:33,903 : INFO : PROGRESS: at 13.88% examples, 522732 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:34,917 : INFO : PROGRESS: at 14.50% examples, 523653 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:35,917 : INFO : PROGRESS: at 15.11% examples, 524502 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:36,927 : INFO : PROGRESS: at 15.84% examples, 527778 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:37,946 : INFO : PROGRESS: at 16.34% examples, 524149 words/s, in_qsize 12, out_qsize 2\n",
      "2017-04-03 09:30:38,954 : INFO : PROGRESS: at 16.77% examples, 519286 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:39,986 : INFO : PROGRESS: at 17.35% examples, 518466 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:41,021 : INFO : PROGRESS: at 17.84% examples, 515257 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:30:42,022 : INFO : PROGRESS: at 18.34% examples, 513337 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:43,057 : INFO : PROGRESS: at 19.05% examples, 515627 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:44,077 : INFO : PROGRESS: at 19.63% examples, 515369 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:45,083 : INFO : PROGRESS: at 20.05% examples, 511113 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:46,096 : INFO : PROGRESS: at 20.66% examples, 511884 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:47,127 : INFO : PROGRESS: at 21.26% examples, 511767 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:48,147 : INFO : PROGRESS: at 21.79% examples, 510325 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:49,158 : INFO : PROGRESS: at 22.46% examples, 512834 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:30:50,180 : INFO : PROGRESS: at 23.14% examples, 514052 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:51,194 : INFO : PROGRESS: at 23.64% examples, 511870 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:52,195 : INFO : PROGRESS: at 24.15% examples, 510501 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:53,220 : INFO : PROGRESS: at 24.60% examples, 507830 words/s, in_qsize 14, out_qsize 3\n",
      "2017-04-03 09:30:54,226 : INFO : PROGRESS: at 25.25% examples, 508858 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:55,255 : INFO : PROGRESS: at 25.79% examples, 507554 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:30:56,265 : INFO : PROGRESS: at 26.29% examples, 505868 words/s, in_qsize 14, out_qsize 2\n",
      "2017-04-03 09:30:57,272 : INFO : PROGRESS: at 26.92% examples, 506470 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:30:58,295 : INFO : PROGRESS: at 27.46% examples, 505237 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:30:59,307 : INFO : PROGRESS: at 27.98% examples, 503751 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:00,313 : INFO : PROGRESS: at 28.54% examples, 503133 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:01,361 : INFO : PROGRESS: at 29.12% examples, 502712 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:02,371 : INFO : PROGRESS: at 29.63% examples, 501445 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:31:03,398 : INFO : PROGRESS: at 30.10% examples, 499726 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:04,410 : INFO : PROGRESS: at 30.66% examples, 499176 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:05,417 : INFO : PROGRESS: at 31.39% examples, 502041 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:06,430 : INFO : PROGRESS: at 31.88% examples, 500914 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:07,453 : INFO : PROGRESS: at 32.36% examples, 499623 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:08,519 : INFO : PROGRESS: at 32.89% examples, 498494 words/s, in_qsize 9, out_qsize 4\n",
      "2017-04-03 09:31:09,519 : INFO : PROGRESS: at 33.45% examples, 498595 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:10,542 : INFO : PROGRESS: at 33.93% examples, 496931 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:11,582 : INFO : PROGRESS: at 34.42% examples, 495607 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:12,595 : INFO : PROGRESS: at 34.88% examples, 494181 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:13,625 : INFO : PROGRESS: at 35.40% examples, 493282 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:14,637 : INFO : PROGRESS: at 35.85% examples, 491464 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:15,643 : INFO : PROGRESS: at 36.52% examples, 492886 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:16,683 : INFO : PROGRESS: at 37.18% examples, 494130 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:17,706 : INFO : PROGRESS: at 37.69% examples, 493340 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:31:18,720 : INFO : PROGRESS: at 38.16% examples, 492127 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:19,740 : INFO : PROGRESS: at 38.56% examples, 490051 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:20,761 : INFO : PROGRESS: at 39.24% examples, 491361 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:31:21,795 : INFO : PROGRESS: at 39.80% examples, 491188 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:22,796 : INFO : PROGRESS: at 40.34% examples, 490941 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:23,827 : INFO : PROGRESS: at 40.85% examples, 490283 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:24,866 : INFO : PROGRESS: at 41.36% examples, 489391 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:25,870 : INFO : PROGRESS: at 41.82% examples, 488297 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:26,875 : INFO : PROGRESS: at 42.53% examples, 490281 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:27,892 : INFO : PROGRESS: at 43.11% examples, 490196 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:28,919 : INFO : PROGRESS: at 43.58% examples, 488923 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:29,943 : INFO : PROGRESS: at 44.09% examples, 488360 words/s, in_qsize 14, out_qsize 2\n",
      "2017-04-03 09:31:30,945 : INFO : PROGRESS: at 44.60% examples, 488010 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:31,963 : INFO : PROGRESS: at 45.09% examples, 487125 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:32,987 : INFO : PROGRESS: at 45.53% examples, 485475 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:34,008 : INFO : PROGRESS: at 45.96% examples, 484147 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:35,010 : INFO : PROGRESS: at 46.61% examples, 485192 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:31:36,062 : INFO : PROGRESS: at 47.06% examples, 483474 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:31:37,088 : INFO : PROGRESS: at 47.58% examples, 482886 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:31:38,112 : INFO : PROGRESS: at 48.14% examples, 482661 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:39,138 : INFO : PROGRESS: at 48.83% examples, 483689 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:31:40,186 : INFO : PROGRESS: at 49.52% examples, 484770 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:41,202 : INFO : PROGRESS: at 50.00% examples, 484106 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:42,210 : INFO : PROGRESS: at 50.51% examples, 483574 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:31:43,213 : INFO : PROGRESS: at 51.03% examples, 483323 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:44,227 : INFO : PROGRESS: at 51.51% examples, 482705 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:45,238 : INFO : PROGRESS: at 51.97% examples, 481949 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:46,245 : INFO : PROGRESS: at 52.42% examples, 481152 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:47,249 : INFO : PROGRESS: at 52.92% examples, 480767 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:48,251 : INFO : PROGRESS: at 53.54% examples, 481378 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:49,273 : INFO : PROGRESS: at 54.17% examples, 482092 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:50,277 : INFO : PROGRESS: at 54.63% examples, 481388 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:51,289 : INFO : PROGRESS: at 55.22% examples, 481683 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:52,337 : INFO : PROGRESS: at 55.96% examples, 482929 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:53,339 : INFO : PROGRESS: at 56.43% examples, 482214 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:54,410 : INFO : PROGRESS: at 56.88% examples, 481137 words/s, in_qsize 14, out_qsize 2\n",
      "2017-04-03 09:31:55,414 : INFO : PROGRESS: at 57.38% examples, 480784 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:31:56,444 : INFO : PROGRESS: at 57.79% examples, 479494 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:57,459 : INFO : PROGRESS: at 58.29% examples, 479257 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:31:58,474 : INFO : PROGRESS: at 58.90% examples, 479695 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:31:59,477 : INFO : PROGRESS: at 59.27% examples, 478168 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:00,495 : INFO : PROGRESS: at 59.58% examples, 476258 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:32:01,513 : INFO : PROGRESS: at 59.94% examples, 474706 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:02,516 : INFO : PROGRESS: at 60.33% examples, 473580 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:32:03,542 : INFO : PROGRESS: at 60.72% examples, 472362 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:04,554 : INFO : PROGRESS: at 61.09% examples, 471035 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:05,573 : INFO : PROGRESS: at 61.54% examples, 470347 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:06,601 : INFO : PROGRESS: at 62.03% examples, 469900 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:07,648 : INFO : PROGRESS: at 62.31% examples, 467811 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:08,659 : INFO : PROGRESS: at 62.59% examples, 465909 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:09,668 : INFO : PROGRESS: at 62.94% examples, 464489 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:10,729 : INFO : PROGRESS: at 63.35% examples, 463319 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:32:11,752 : INFO : PROGRESS: at 63.75% examples, 462313 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:12,769 : INFO : PROGRESS: at 64.12% examples, 461112 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:32:13,777 : INFO : PROGRESS: at 64.52% examples, 460248 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:14,790 : INFO : PROGRESS: at 64.91% examples, 459320 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:15,843 : INFO : PROGRESS: at 65.33% examples, 458250 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:16,849 : INFO : PROGRESS: at 65.68% examples, 457088 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:17,850 : INFO : PROGRESS: at 66.15% examples, 456630 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:18,895 : INFO : PROGRESS: at 66.69% examples, 456641 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:19,904 : INFO : PROGRESS: at 67.06% examples, 455325 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:20,914 : INFO : PROGRESS: at 67.38% examples, 453977 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:21,920 : INFO : PROGRESS: at 67.72% examples, 452731 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:22,939 : INFO : PROGRESS: at 68.07% examples, 451449 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:23,947 : INFO : PROGRESS: at 68.39% examples, 450064 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:32:24,985 : INFO : PROGRESS: at 68.74% examples, 448771 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:26,023 : INFO : PROGRESS: at 69.07% examples, 447501 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:27,039 : INFO : PROGRESS: at 69.42% examples, 446376 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:32:28,055 : INFO : PROGRESS: at 69.94% examples, 446389 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:29,113 : INFO : PROGRESS: at 70.33% examples, 445509 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:30,208 : INFO : PROGRESS: at 70.66% examples, 444004 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:31,213 : INFO : PROGRESS: at 70.96% examples, 442812 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:32,233 : INFO : PROGRESS: at 71.28% examples, 441595 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:33,254 : INFO : PROGRESS: at 71.61% examples, 440531 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:34,267 : INFO : PROGRESS: at 71.91% examples, 439359 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:35,286 : INFO : PROGRESS: at 72.35% examples, 439040 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:32:36,327 : INFO : PROGRESS: at 72.94% examples, 439519 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:37,332 : INFO : PROGRESS: at 73.34% examples, 438950 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:38,391 : INFO : PROGRESS: at 73.75% examples, 438232 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:32:39,393 : INFO : PROGRESS: at 74.17% examples, 437735 words/s, in_qsize 10, out_qsize 3\n",
      "2017-04-03 09:32:40,406 : INFO : PROGRESS: at 74.56% examples, 437094 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:41,446 : INFO : PROGRESS: at 74.97% examples, 436496 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:42,452 : INFO : PROGRESS: at 75.39% examples, 436008 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:43,501 : INFO : PROGRESS: at 75.83% examples, 435422 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:44,576 : INFO : PROGRESS: at 76.25% examples, 434754 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:45,590 : INFO : PROGRESS: at 76.64% examples, 434146 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:46,609 : INFO : PROGRESS: at 77.00% examples, 433421 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:47,640 : INFO : PROGRESS: at 77.40% examples, 432814 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:48,659 : INFO : PROGRESS: at 77.77% examples, 432107 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:49,669 : INFO : PROGRESS: at 78.25% examples, 432124 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:32:50,677 : INFO : PROGRESS: at 78.78% examples, 432332 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:32:51,677 : INFO : PROGRESS: at 79.10% examples, 431275 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:52,692 : INFO : PROGRESS: at 79.41% examples, 430330 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:53,721 : INFO : PROGRESS: at 79.75% examples, 429406 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:54,726 : INFO : PROGRESS: at 80.08% examples, 428557 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:55,734 : INFO : PROGRESS: at 80.48% examples, 428094 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:56,744 : INFO : PROGRESS: at 80.85% examples, 427512 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:57,751 : INFO : PROGRESS: at 81.21% examples, 426843 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:58,771 : INFO : PROGRESS: at 81.61% examples, 426339 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:32:59,772 : INFO : PROGRESS: at 82.17% examples, 426754 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:00,817 : INFO : PROGRESS: at 82.52% examples, 425974 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:01,844 : INFO : PROGRESS: at 82.93% examples, 425477 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:03,003 : INFO : PROGRESS: at 83.29% examples, 424405 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:04,023 : INFO : PROGRESS: at 83.58% examples, 423344 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:05,053 : INFO : PROGRESS: at 83.88% examples, 422396 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:33:06,068 : INFO : PROGRESS: at 84.18% examples, 421462 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:07,198 : INFO : PROGRESS: at 84.52% examples, 420503 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:08,205 : INFO : PROGRESS: at 85.02% examples, 420551 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:09,220 : INFO : PROGRESS: at 85.55% examples, 420646 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:10,356 : INFO : PROGRESS: at 85.91% examples, 419806 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:11,418 : INFO : PROGRESS: at 86.20% examples, 418740 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:12,426 : INFO : PROGRESS: at 86.55% examples, 418095 words/s, in_qsize 14, out_qsize 3\n",
      "2017-04-03 09:33:13,444 : INFO : PROGRESS: at 86.99% examples, 417717 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:14,456 : INFO : PROGRESS: at 87.38% examples, 417280 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:15,469 : INFO : PROGRESS: at 87.76% examples, 416735 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:16,470 : INFO : PROGRESS: at 88.14% examples, 416219 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:17,491 : INFO : PROGRESS: at 88.51% examples, 415594 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:18,504 : INFO : PROGRESS: at 88.85% examples, 414912 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:19,531 : INFO : PROGRESS: at 89.35% examples, 414952 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:20,547 : INFO : PROGRESS: at 89.68% examples, 414282 words/s, in_qsize 8, out_qsize 5\n",
      "2017-04-03 09:33:21,554 : INFO : PROGRESS: at 89.93% examples, 413249 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:22,573 : INFO : PROGRESS: at 90.42% examples, 413270 words/s, in_qsize 11, out_qsize 2\n",
      "2017-04-03 09:33:23,690 : INFO : PROGRESS: at 90.93% examples, 413227 words/s, in_qsize 14, out_qsize 1\n",
      "2017-04-03 09:33:24,705 : INFO : PROGRESS: at 91.30% examples, 412808 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:25,761 : INFO : PROGRESS: at 91.68% examples, 412361 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:26,791 : INFO : PROGRESS: at 92.04% examples, 411840 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:33:27,793 : INFO : PROGRESS: at 92.37% examples, 411232 words/s, in_qsize 14, out_qsize 2\n",
      "2017-04-03 09:33:28,871 : INFO : PROGRESS: at 92.72% examples, 410621 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:29,896 : INFO : PROGRESS: at 93.07% examples, 410051 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:30,898 : INFO : PROGRESS: at 93.38% examples, 409416 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:33:31,905 : INFO : PROGRESS: at 93.72% examples, 408825 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:33:32,911 : INFO : PROGRESS: at 94.09% examples, 408345 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:33,919 : INFO : PROGRESS: at 94.44% examples, 407855 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:34,921 : INFO : PROGRESS: at 94.83% examples, 407596 words/s, in_qsize 12, out_qsize 1\n",
      "2017-04-03 09:33:35,923 : INFO : PROGRESS: at 95.20% examples, 407200 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:36,967 : INFO : PROGRESS: at 95.64% examples, 406919 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:38,043 : INFO : PROGRESS: at 96.02% examples, 406401 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:39,092 : INFO : PROGRESS: at 96.42% examples, 406043 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:40,132 : INFO : PROGRESS: at 96.82% examples, 405749 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:41,143 : INFO : PROGRESS: at 97.11% examples, 405020 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:42,148 : INFO : PROGRESS: at 97.47% examples, 404647 words/s, in_qsize 10, out_qsize 3\n",
      "2017-04-03 09:33:43,203 : INFO : PROGRESS: at 97.86% examples, 404258 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:44,215 : INFO : PROGRESS: at 98.23% examples, 403886 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:45,273 : INFO : PROGRESS: at 98.64% examples, 403596 words/s, in_qsize 14, out_qsize 0\n",
      "2017-04-03 09:33:46,348 : INFO : PROGRESS: at 99.04% examples, 403179 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:47,374 : INFO : PROGRESS: at 99.41% examples, 402754 words/s, in_qsize 10, out_qsize 3\n",
      "2017-04-03 09:33:48,399 : INFO : PROGRESS: at 99.80% examples, 402463 words/s, in_qsize 13, out_qsize 0\n",
      "2017-04-03 09:33:48,843 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-03 09:33:48,853 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-03 09:33:48,873 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-03 09:33:48,896 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-03 09:33:48,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-03 09:33:48,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-03 09:33:48,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-03 09:33:48,937 : INFO : training on 117941515 raw words (87816610 effective words) took 218.3s, 402257 effective words/s\n",
      "2017-04-03 09:33:48,954 : INFO : saving Word2Vec object under C:/Users/Yash/Documents/Project_4/imdb1.w2v, separately None\n",
      "2017-04-03 09:33:48,960 : INFO : not storing attribute syn0norm\n",
      "2017-04-03 09:33:48,963 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-03 09:33:52,198 : INFO : saved C:/Users/Yash/Documents/Project_4/imdb1.w2v\n",
      "2017-04-03 09:33:52,202 : INFO : loading Word2Vec object from C:/Users/Yash/Documents/Project_4/imdb1.w2v\n",
      "2017-04-03 09:33:54,656 : INFO : loading wv recursively from C:/Users/Yash/Documents/Project_4/imdb1.w2v.wv.* with mmap=None\n",
      "2017-04-03 09:33:54,658 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-03 09:33:54,661 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-03 09:33:54,663 : INFO : loaded C:/Users/Yash/Documents/Project_4/imdb1.w2v\n",
      "2017-04-03 09:33:55,011 : INFO : storing 53263x100 projection weights into C:/Users/Yash/Documents/Project_4/imdb_readable.w2v\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []\n",
    "count = 0\n",
    "\n",
    "for document in training_list + testing_list + unsupervised_list:\n",
    "    sentences.append(document.split())\n",
    "    \n",
    "print('Word2Vec generation started ...')\n",
    "model = gensim.models.Word2Vec(sentences, min_count=5, window=10, size=100, workers=7)\n",
    "print('Saving the model')\n",
    "model.save('resources/imdb.w2v')\n",
    "model = gensim.models.Word2Vec.load('resources/imdb.w2v')\n",
    "model.wv.save_word2vec_format('resources/imdb_readable.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-03 09:34:05,580 : INFO : storing 53263x100 projection weights into C:/Users/Yash/Documents/Project_4/wordvec_pre.txt\n"
     ]
    }
   ],
   "source": [
    "model.wv.save_word2vec_format('resources/wordvec_pre.txt', binary=False)\n",
    "import numpy as np\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.wv = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        #self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.wv[w] for w in words if w in self.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "[[ 0.78009146  0.87203753 -0.53153795 ...,  0.38531545  0.89650971\n",
      "  -0.34766012]\n",
      " [ 0.87291992  0.81601912 -0.7513985  ...,  0.25126147  0.70795953\n",
      "  -0.03906186]\n",
      " [ 0.44415048  0.71300572 -0.51766372 ...,  0.22169966  0.45642874\n",
      "  -0.04845271]\n",
      " ..., \n",
      " [ 0.69752133  1.08358967 -0.46106651 ...,  0.3044163   0.64457202\n",
      "  -0.60528517]\n",
      " [ 0.37689963  0.9939642  -0.42636824 ...,  0.41567418  0.72116739\n",
      "  -0.37780032]\n",
      " [ 0.41763598  1.3784945  -0.85002297 ...,  0.37775347  0.63217473\n",
      "  -0.90100771]]\n"
     ]
    }
   ],
   "source": [
    "w2v = MeanEmbeddingVectorizer(model)\n",
    "y_train = []\n",
    "y_test = []\n",
    "X_train = sentences[ : 25000]\n",
    "X_test = sentences[25000:50000]\n",
    "\n",
    "for i in range(12500):\n",
    "    y_train.append('positive')\n",
    "    y_test.append('positive')\n",
    "\n",
    "for i in range(12500):\n",
    "    y_train.append('negative')\n",
    "    y_test.append('negative')\n",
    "    \n",
    "X_train = w2v.transform(X_train)\n",
    "X_test=w2v.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier Training starts ...\n",
      "Logistic Regression Classifier Training is done and transorming testing data using trained tf-idf ...\n",
      "Testing Data Prediction Done!!!\n",
      "['positive' 'positive' 'positive' ..., 'positive' 'positive' 'negative']\n",
      "0.85384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model_LR1 = LogisticRegression()\n",
    "print('Logistic Regression Classifier Training starts ...')\n",
    "model_LR1 = model_LR1.fit(X_train, y_train)\n",
    "print('Logistic Regression Classifier Training is done and transorming testing data using trained tf-idf ...')\n",
    "\n",
    "\n",
    "\n",
    "y_pred1 = model_LR1.predict(X_test)\n",
    "\n",
    "print('Testing Data Prediction Done!!!')\n",
    "\n",
    "print(y_pred1)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classifier Training starts ...\n",
      "Support Vector Machine Classifier Training is done and transorming testing data using trained tf-idf ...\n",
      "Testing Data Prediction Done!!!\n",
      "(25000,)\n",
      "['positive' 'positive' 'positive' ..., 'positive' 'positive' 'negative']\n",
      "0.8536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_lSVC = LinearSVC()\n",
    "print('Support Vector Machine Classifier Training starts ...')\n",
    "model_lSVC.fit(X_train, y_train)  \n",
    "print('Support Vector Machine Classifier Training is done and transorming testing data using trained tf-idf ...')\n",
    "\n",
    "\n",
    "y_pred = model_lSVC.predict(X_test)\n",
    "print('Testing Data Prediction Done!!!')\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Classifier Training starts ...\n",
      "Stochastic Gradient Descent Classifier Training is done and transorming testing data using trained tf-idf ...\n",
      "Testing Data Prediction Done!!!\n",
      "(25000,)\n",
      "['positive' 'positive' 'positive' ..., 'negative' 'positive' 'negative']\n",
      "0.84608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_SGD = SGDClassifier()\n",
    "print('Stochastic Gradient Descent Classifier Training starts ...')\n",
    "model_SGD.fit(X_train, y_train)\n",
    "print('Stochastic Gradient Descent Classifier Training is done and transorming testing data using trained tf-idf ...')\n",
    "\n",
    "\n",
    "y_pred = model_SGD.predict(X_test)\n",
    "print('Testing Data Prediction Done!!!')\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier using gini index Training starts ...\n",
      "Decision Tree Classifier Training is done and transorming testing data using trained tf-idf ...\n",
      "Testing Data Prediction Done!!!\n",
      "(25000,)\n",
      "['positive' 'positive' 'positive' ..., 'positive' 'positive' 'negative']\n",
      "0.68472\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model_DT = tree.DecisionTreeClassifier(criterion='gini')\n",
    "print('Decision Tree Classifier using gini index Training starts ...')\n",
    "model_DT = model_DT.fit(X_train, y_train)\n",
    "print('Decision Tree Classifier Training is done and transorming testing data using trained tf-idf ...')\n",
    "\n",
    "\n",
    "y_pred = model_DT.predict(X_test)\n",
    "print('Testing Data Prediction Done!!!')\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier using 10 n_estimatores Training starts ...\n",
      "Gradient Boosting Classifier Training is done and transorming testing data using trained tf-idf ...\n",
      "Testing Data Prediction Done!!!\n",
      "(25000,)\n",
      "['positive' 'positive' 'positive' ..., 'positive' 'positive' 'positive']\n",
      "0.8352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_GBC = GradientBoostingClassifier(n_estimators=1000, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "print('Gradient Boosting Classifier using 10 n_estimatores Training starts ...')\n",
    "model_GBC = model_GBC.fit(X_train, y_train)\n",
    "print('Gradient Boosting Classifier Training is done and transorming testing data using trained tf-idf ...')\n",
    "\n",
    "\n",
    "y_pred = model_GBC.predict(X_test)\n",
    "print('Testing Data Prediction Done!!!')\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
